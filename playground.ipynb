{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2724c7b6-3913-4e88-b25b-94e840895c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/dmpowell/.cache/huggingface\n",
      "/scratch/dmpowell/.cache/huggingface/datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmpowell/.conda/envs/py3.11transformers4.44/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device =  cuda\n"
     ]
    }
   ],
   "source": [
    "## ---------------------------------------------------------------------\n",
    "## set up configs for huggingface hub and OS paths on HPC cluster -- make sure config.ini is correct\n",
    "## ---------------------------------------------------------------------\n",
    "import configparser\n",
    "\n",
    "def scratch_path():\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(\"config.ini\")\n",
    "    return \"/scratch/\" + config[\"user\"][\"username\"]\n",
    "\n",
    "import os\n",
    "if os.path.isdir(scratch_path()):\n",
    "    os.environ['TRANSFORMERS_CACHE'] = scratch_path() + '/.cache/huggingface'\n",
    "    os.environ['HF_DATASETS_CACHE'] = scratch_path() + '/.cache/huggingface/datasets'\n",
    "print(os.getenv('TRANSFORMERS_CACHE'))\n",
    "print(os.getenv('HF_DATASETS_CACHE'))\n",
    "\n",
    "## ---------------------------------------------------------------------\n",
    "## Load libraries\n",
    "## ---------------------------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel, LlamaForCausalLM, LlamaTokenizer, AutoModelForCausalLM\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from baukit import Trace\n",
    "\n",
    "from steering import *\n",
    "## ---------------------------------------------------------------------\n",
    "## Ensure GPU is available -- device should == 'cuda'\n",
    "## ---------------------------------------------------------------------\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device = \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de83defa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb42d7e23a1e4562853deef13f36c5cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb262ca3a4594bf8bd3a1d8f78629372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "# MODEL_NAME = \"meta-llama/Llama-3.1-8B\"\n",
    "\n",
    "wmodel = SteeringModel(\n",
    "    AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,  # Replace this with the 70B variant if available\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=device  # Automatically distributes the model across available GPUs\n",
    "    ),\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, device = 'cuda', use_fast = False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5cb158",
   "metadata": {},
   "source": [
    "## Multiple choice\n",
    "\n",
    "Here is a basic implementation of multiple choice answering using \"cloze\" probabilities. This should roughly work with both raw and instruction-tuned models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "473a7440",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def answer_choice_list(choices):\n",
    "    options = re.split(r'\\s*\\(\\w\\)\\s*', choices)\n",
    "    return( [option.strip() for option in options if option] )\n",
    "\n",
    "\n",
    "def format_question(question):\n",
    "    return f\"Q: {question}\\nA:\"\n",
    "\n",
    "\n",
    "# def format_statement(question, choices):\n",
    "#     choice_string = \", \".join(choices)\n",
    "#     return f\"Please rate your agreement with the following statement, using the following scale: [{choice_string}]. Statement: {question}\\nResponse:\"\n",
    "\n",
    "\n",
    "def format_with_instructions(instruction, question, choices):\n",
    "    # choice_string = \"; \".join(choices)\n",
    "    return f\"{instruction} Specifically, please use the following response options: {choices}.\\n\\nStatement: {question}\\nResponse:\"\n",
    "\n",
    "\n",
    "def format_with_mcqa_instructions(instruction, question, choices_text):\n",
    "    \n",
    "    LETTERS = [chr(i) for i in range(65,91)]\n",
    "    choices = re.split(';\\W', choices_text)\n",
    "    choices = [c.strip() for c in choices]\n",
    "    labeled_choices = [\". \".join([a,b]) for a, b in zip(LETTERS, choices)]\n",
    "    labeled_choices = \"\\n\".join(labeled_choices)\n",
    "    \n",
    "    return f\"{instruction} Respond with the letter corresponding to your choice from the following response options:\\n\\n{labeled_choices}\\n\\nStatement: {question}\\nResponse:\"\n",
    "\n",
    "\n",
    "def format_chat_question(instruction, question, choices):\n",
    "    return f\"{instruction} Specifically, please use the following response options: {choices}.\\n\\nStatement: {question}\"\n",
    "\n",
    "\n",
    "def format_mcqa_chat_question(instruction, question, choices_text):\n",
    "    \n",
    "    LETTERS = [chr(i) for i in range(65,91)]\n",
    "    choices = re.split(';\\W', choices_text)\n",
    "    choices = [c.strip() for c in choices]\n",
    "    labeled_choices = [\". \".join([a,b]) for a, b in zip(LETTERS, choices)]\n",
    "    labeled_choices = \"\\n\".join(labeled_choices)\n",
    "    \n",
    "    return f\"{instruction} Respond with the letter corresponding to your choice from the following response options:\\n\\n{labeled_choices}\\n\\nStatement: {question}\"\n",
    "\n",
    "\n",
    "def format_chat(instruction, question, choices):\n",
    "\n",
    "    chat = [\n",
    "        {\"role\": \"user\", \"content\": format_chat_question(instruction, question, choices)},\n",
    "        {\"role\": \"system\", \"content\": \"My Response:\"}\n",
    "    ]\n",
    "\n",
    "    tokens = wmodel.tok.apply_chat_template(chat, tokenize=True, continue_final_message=True)[:-1]\n",
    "\n",
    "    return(wmodel.tok.decode(tokens))\n",
    "\n",
    "\n",
    "def format_mcqa_chat(instruction, question, choices):\n",
    "\n",
    "    chat = [\n",
    "        {\"role\": \"user\", \"content\": format_mcqa_chat_question(instruction, question, choices)},\n",
    "        {\"role\": \"system\", \"content\": \"My Response:\"}\n",
    "    ]\n",
    "\n",
    "    tokens = wmodel.tok.apply_chat_template(chat, tokenize=True, continue_final_message=True)[:-1]\n",
    "\n",
    "    return(wmodel.tok.decode(tokens))\n",
    "\n",
    "\n",
    "def mc_choice_probs(model, question, choices, pad = True):\n",
    "    prompt = question\n",
    "    if pad:\n",
    "        choices = [\" \" + c for c in choices] # pad all the \n",
    "    \n",
    "    prompts = [prompt for c in choices]\n",
    "    \n",
    "    logits = torch.tensor([model.completion_logprob(x[0], x[1]) for x in zip(prompts, choices)])\n",
    "    \n",
    "    return(F.log_softmax(logits, -1).exp())\n",
    "\n",
    "\n",
    "def choice_score(choice_probs):\n",
    "    # calculate score on -1 to 1 scale\n",
    "    choice_score01 = choice_probs @ torch.arange(len(choice_probs), dtype = choice_probs.dtype)/(len(choice_probs)-1)\n",
    "    return (choice_score01.item() - .5)*2\n",
    "\n",
    "\n",
    "def get_mean_steering_vector(ziplist, model, layer_id = 15):\n",
    "    return torch.concat([model.get_steering_vector(s1, s2, layer_id) for s1, s2 in ziplist]).mean(0).unsqueeze(0)\n",
    "\n",
    "\n",
    "def act_add(steering_vec):\n",
    "    def act_add(output):\n",
    "        z = torch.ones_like(output[0])\n",
    "        z = z * steering_vec\n",
    "        out = output[0] + z\n",
    "        return (out, output[1])\n",
    "    return act_add"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7806dfd9",
   "metadata": {},
   "source": [
    "For any agree/disagree etc. style scales, we can take the choice probabilities and compute a \"score\". I noticed the model seems to have a really strong \"agree\" bias when we have a pure \"agree\" option. Will need to look into this, probably some literature on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ecca48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choice_text = ['Strongly disagree', 'Somewhat disagree', \"Neither agree nor disagree\", 'Somewhat agree', 'Strongly agree']\n",
    "# # q = format_question('Slavery benefitted the slaves, many of whom learned valuable skills.')\n",
    "# q = format_statement('Slavery benefitted the slaves, many of whom learned valuable skills.', choice_text)\n",
    "# choice_probs = mc_choice_probs(wmodel, q, choice_text )\n",
    "# choice_score(choice_probs), choice_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0229b7e",
   "metadata": {},
   "source": [
    "## Steering\n",
    "\n",
    "Applying a steering vector shifts generations ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99a54164",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In my opinion, dogs are the most wonderful creatures on this earth. They are loyal, loving, and always happy to see']\n",
      "--------------------\n",
      "['In my opinion, dogs are one of the most loyal and loving animals on the planet. They are a true companion and are']\n",
      "--------------------\n",
      "['In my opinion, dogs are the most disgusting things in the world. They are disgusting, filthy, and dise-ridden.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "steering_vec = get_mean_steering_vector([[\"Love\", \"Hate\"], [\"Beautiful\", \"Disgusting\"], [\"Loving\", \"Hateful\"]], wmodel)\n",
    "\n",
    "test_sentence = \"In my opinion, dogs\"\n",
    "\n",
    "print(wmodel.generate(test_sentence, max_new_tokens = 20))\n",
    "print(\"-\"*20)\n",
    "with Trace(wmodel.get_module(), edit_output = act_add(steering_vec)):\n",
    "    print(wmodel.generate(test_sentence, max_new_tokens = 20))\n",
    "print(\"-\"*20)\n",
    "with Trace(wmodel.get_module(), edit_output = act_add(-1.5*steering_vec)):\n",
    "    print(wmodel.generate(test_sentence, max_new_tokens = 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc77d7fe",
   "metadata": {},
   "source": [
    "### \"survey\" responses and attitude scores\n",
    "\n",
    "Can apply the same thing to our multiple choice decisions and calculated scores. Adding a relevant steering vector affects the responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3bd318be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0900799036026001 tensor([0.4413, 0.0123, 0.0035, 0.0108, 0.5321])\n",
      "-0.8198438286781311 tensor([0.9074, 0.0010, 0.0026, 0.0018, 0.0872])\n"
     ]
    }
   ],
   "source": [
    "steering_vec = wmodel.get_steering_vector(\"Slavery is a proud part of American history.\", \"Slavery is the most shameful part of America's past.\")\n",
    "\n",
    "# steering_vec = get_mean_steering_vector([[\"Some races are inferior to others.\", \"No race is inferior to others.\"], [\"White people are superior to black people.\", \"White and black people are equally good.\"]], wmodel)\n",
    "\n",
    "# q = format_question('Slavery benefitted the slaves, many of whom learned valuable skills.')\n",
    "q = format_statement('Slavery benefitted the slaves, many of whom learned valuable skills.', choice_text)\n",
    "\n",
    "with Trace(wmodel.get_module(), edit_output = act_add(steering_vec)):\n",
    "    # print(wmodel.generate(test_sentence, max_new_tokens = 20))\n",
    "    choice_probs = mc_choice_probs(wmodel, format_question('Slavery benefitted the slaves who learned valuable skills.'), ['Strongly disagree',  'Somewhat disagree', \"Neither agree nor disagree\", 'Somewhat agree', 'Strongly agree'])\n",
    "    print(choice_score(choice_probs), choice_probs) \n",
    "\n",
    "with Trace(wmodel.get_module(), edit_output = act_add(-steering_vec)):\n",
    "    # print(wmodel.generate(test_sentence, max_new_tokens = 20))\n",
    "    choice_probs = mc_choice_probs(wmodel, format_question('Slavery benefitted the slaves who learned valuable skills.'), ['Strongly disagree',  'Somewhat disagree', \"Neither agree nor disagree\", 'Somewhat agree', 'Strongly agree'])\n",
    "    print(choice_score(choice_probs), choice_probs) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed5efa7",
   "metadata": {},
   "source": [
    "## Applying to survey ideology scales\n",
    "\n",
    "First, to generate the model's answers.\n",
    "\n",
    "I think the overall wisest way to go about things is with MCQA wih the instruct model. But interesting also with the non-instruct model, but we need a good few-shot prompt for the MCQA behavior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32ecfb94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      (1) Very negative; (2) Negative; (3) Slightly ...\n",
       "1      (1) Very negative; (2) Negative; (3) Slightly ...\n",
       "2      (1) Very negative; (2) Negative; (3) Slightly ...\n",
       "3      (1) Very negative; (2) Negative; (3) Slightly ...\n",
       "4      (1) Very negative; (2) Negative; (3) Slightly ...\n",
       "                             ...                        \n",
       "265    (1) Strongly disagree; (2) Disagree; (3) Neutr...\n",
       "266    (1) Strongly disagree; (2) Disagree; (3) Neutr...\n",
       "267    (1) Strongly disagree; (2) Disagree; (3) Neutr...\n",
       "268    (1) Strongly disagree; (2) Disagree; (3) Neutr...\n",
       "269    (1) Strongly disagree; (2) Disagree; (3) Neutr...\n",
       "Name: response_options, Length: 270, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scales = pd.read_csv(\"data/scales.tsv\", sep=\"\\t\")\n",
    "\n",
    "scales = scales.loc[lambda x: x.sub_scale != 'not scored']\n",
    "scales['response_options'] = [re.sub(r\"\\s*\\(.*?\\)\\s*\", \" \", text).strip() for text in scales['response_options']]\n",
    "scales['statement'] = [text.strip() for text in scales['statement']]\n",
    "scales['simple_contrastive_statement'] = [text.strip() for text in scales['simple_contrastive_statement']]\n",
    "\n",
    "scales['statement'] = [text + \".\" if text[-1]!=\".\" else text for text in scales['statement']]\n",
    "scales['simple_contrastive_statement'] = [text + \".\" if text[-1]!=\".\" else text for text in scales['simple_contrastive_statement'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cdb383c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m MODEL_NAME\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Llama-3.1-8B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     25\u001b[0m         q \u001b[38;5;241m=\u001b[39m format_chat(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstruction\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse_options\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 27\u001b[0m choice_probs \u001b[38;5;241m=\u001b[39m mc_choice_probs(wmodel, q, choices) \u001b[38;5;66;03m# format_chat for instruct model\u001b[39;00m\n\u001b[1;32m     29\u001b[0m resp_probs\u001b[38;5;241m.\u001b[39mappend(choice_probs\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     30\u001b[0m resps\u001b[38;5;241m.\u001b[39mappend(choice_score(choice_probs) \u001b[38;5;28;01mif\u001b[39;00m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdirection\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhigh\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39mchoice_score(choice_probs))\n",
      "Cell \u001b[0;32mIn[5], line 79\u001b[0m, in \u001b[0;36mmc_choice_probs\u001b[0;34m(model, question, choices, pad)\u001b[0m\n\u001b[1;32m     75\u001b[0m     choices \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m choices] \u001b[38;5;66;03m# pad all the \u001b[39;00m\n\u001b[1;32m     77\u001b[0m prompts \u001b[38;5;241m=\u001b[39m [prompt \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m choices]\n\u001b[0;32m---> 79\u001b[0m logits \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([model\u001b[38;5;241m.\u001b[39mcompletion_logprob(x[\u001b[38;5;241m0\u001b[39m], x[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(prompts, choices)])\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m(F\u001b[38;5;241m.\u001b[39mlog_softmax(logits, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mexp())\n",
      "Cell \u001b[0;32mIn[5], line 79\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     75\u001b[0m     choices \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m choices] \u001b[38;5;66;03m# pad all the \u001b[39;00m\n\u001b[1;32m     77\u001b[0m prompts \u001b[38;5;241m=\u001b[39m [prompt \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m choices]\n\u001b[0;32m---> 79\u001b[0m logits \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([model\u001b[38;5;241m.\u001b[39mcompletion_logprob(x[\u001b[38;5;241m0\u001b[39m], x[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(prompts, choices)])\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m(F\u001b[38;5;241m.\u001b[39mlog_softmax(logits, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mexp())\n",
      "File \u001b[0;32m~/work/ideology-steering/steering/steering.py:93\u001b[0m, in \u001b[0;36mSteeringModel.completion_logprob\u001b[0;34m(self, prefix, suffix, summed)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompletion_logprob\u001b[39m(\u001b[38;5;28mself\u001b[39m, prefix, suffix, summed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     92\u001b[0m     full_text \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;241m+\u001b[39m suffix\n\u001b[0;32m---> 93\u001b[0m     full \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_logprobs(full_text)\n\u001b[1;32m     94\u001b[0m     pre_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtok(prefix)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     95\u001b[0m     res \u001b[38;5;241m=\u001b[39m full[(pre_len\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m summed \u001b[38;5;28;01melse\u001b[39;00m full[(pre_len\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):]\u001b[38;5;241m.\u001b[39msum()\n",
      "File \u001b[0;32m~/work/ideology-steering/steering/steering.py:85\u001b[0m, in \u001b[0;36mSteeringModel.obs_logprobs\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobs_logprobs\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[0;32m---> 85\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_logits(text)\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [F\u001b[38;5;241m.\u001b[39mlog_softmax(l, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m logits] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(logits)\u001b[38;5;241m==\u001b[39m\u001b[38;5;28mlist\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlog_softmax(logits, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/work/ideology-steering/steering/steering.py:64\u001b[0m, in \u001b[0;36mSteeringModel.obs_logits\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobs_logits\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[0;32m---> 64\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogits(text)\n\u001b[1;32m     65\u001b[0m     logits \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     67\u001b[0m     obslogits \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/work/ideology-steering/steering/steering.py:49\u001b[0m, in \u001b[0;36mSteeringModel.logits\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# texts = self.preprompt + texts if type(texts)==str else [self.preprompt + t for t in texts]\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \n\u001b[1;32m     45\u001b[0m \n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# encoding = self.tok(texts, padding=True, return_tensors='pt').to(self.model.device)\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 49\u001b[0m     model_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(encoding[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     50\u001b[0m     logits \u001b[38;5;241m=\u001b[39m model_out\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: encoding, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m: logits}\n",
      "File \u001b[0;32m~/.conda/envs/py3.11transformers4.44/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/py3.11transformers4.44/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/py3.11transformers4.44/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1189\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1186\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1189\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m   1190\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1191\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   1192\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   1193\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m   1194\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m   1195\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1196\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1197\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1198\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1199\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m   1200\u001b[0m )\n\u001b[1;32m   1202\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/py3.11transformers4.44/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/py3.11transformers4.44/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/py3.11transformers4.44/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1001\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    989\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    990\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    991\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    998\u001b[0m         position_embeddings,\n\u001b[1;32m    999\u001b[0m     )\n\u001b[1;32m   1000\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1001\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[1;32m   1002\u001b[0m         hidden_states,\n\u001b[1;32m   1003\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[1;32m   1004\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   1005\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m   1006\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1007\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1008\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m   1009\u001b[0m         position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[1;32m   1010\u001b[0m     )\n\u001b[1;32m   1012\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.conda/envs/py3.11transformers4.44/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/py3.11transformers4.44/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/py3.11transformers4.44/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:749\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n\u001b[1;32m    748\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m--> 749\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[1;32m    750\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(hidden_states)\n\u001b[1;32m    751\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/.conda/envs/py3.11transformers4.44/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/py3.11transformers4.44/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/py3.11transformers4.44/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:124\u001b[0m, in \u001b[0;36mLlamaRMSNorm.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    122\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    123\u001b[0m variance \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 124\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mrsqrt(variance \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariance_epsilon)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m*\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(input_dtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Getting model responses\n",
    "MCQA = True\n",
    "resps = []\n",
    "resp_probs = []\n",
    "\n",
    "for idx, row in scales.iterrows():\n",
    "\n",
    "    if MCQA:\n",
    "        LETTERS = [chr(i) for i in range(65,91)]\n",
    "        choices = re.split(';\\W', row['response_options'])\n",
    "        choices = LETTERS[:len(choices)]\n",
    "        \n",
    "        if MODEL_NAME==\"meta-llama/Llama-3.1-8B\":\n",
    "            q = format_with_mcqa_instructions(row['instruction'], row['question'], row['response_options'])\n",
    "        elif MODEL_NAME==\"meta-llama/Llama-3.1-8B-Instruct\":\n",
    "            q = format_with_mcqa_instructions(row['instruction'], row['question'], row['response_options']) # format_mcqa_chat\n",
    "    \n",
    "    else:\n",
    "        choices = re.split(';\\W', row['response_options'])\n",
    "        choices = [c.strip() for c in choices]\n",
    "\n",
    "        if MODEL_NAME==\"meta-llama/Llama-3.1-8B\":\n",
    "            q = format_with_instructions(row['instruction'], row['question'], row['response_options'])\n",
    "        elif MODEL_NAME==\"meta-llama/Llama-3.1-8B-Instruct\":\n",
    "            q = format_chat(row['instruction'], row['question'], row['response_options'])\n",
    "    \n",
    "    choice_probs = mc_choice_probs(wmodel, q, choices) # format_chat for instruct model\n",
    "\n",
    "    resp_probs.append(choice_probs.detach().numpy())\n",
    "    resps.append(choice_score(choice_probs) if row['direction']=='high' else -choice_score(choice_probs))\n",
    "    \n",
    "scales[\"response_probs\"] = resp_probs\n",
    "scales[\"model_score\"] = resps\n",
    "\n",
    "scales.groupby(['scale', 'sub_scale']).agg(avg_score = ('model_score', 'mean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3322e4b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item</th>\n",
       "      <th>instruction</th>\n",
       "      <th>original_instruction</th>\n",
       "      <th>question</th>\n",
       "      <th>statement</th>\n",
       "      <th>simple_contrastive_statement</th>\n",
       "      <th>strong_constrastive_statement</th>\n",
       "      <th>response_options</th>\n",
       "      <th>scale</th>\n",
       "      <th>construct</th>\n",
       "      <th>original_item</th>\n",
       "      <th>direction</th>\n",
       "      <th>sub_scale</th>\n",
       "      <th>explicit_social_bias</th>\n",
       "      <th>response_probs</th>\n",
       "      <th>model_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>When you decide whether something is right or ...</td>\n",
       "      <td>True</td>\n",
       "      <td>Whether or not someone suffered emotionally</td>\n",
       "      <td>I care a lot about someone suffering emotionally.</td>\n",
       "      <td>I don't care at all about someone suffering em...</td>\n",
       "      <td>Whether or not someone suffered emotionally ha...</td>\n",
       "      <td>Not at all relevant; Not very relevant; Slight...</td>\n",
       "      <td>MFQ</td>\n",
       "      <td>Moral Foundations</td>\n",
       "      <td>True</td>\n",
       "      <td>high</td>\n",
       "      <td>Harm</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>[0.01597328, 0.11802746, 0.2205043, 0.32083187...</td>\n",
       "      <td>0.169737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>23</td>\n",
       "      <td>When you decide whether something is right or ...</td>\n",
       "      <td>True</td>\n",
       "      <td>Whether or not someone cared for someone weak ...</td>\n",
       "      <td>I care a lot about caring for the weak and vul...</td>\n",
       "      <td>I don't care at all about caring for the weak ...</td>\n",
       "      <td>Whether or not someone cared for someone weak ...</td>\n",
       "      <td>Not at all relevant; Not very relevant; Slight...</td>\n",
       "      <td>MFQ</td>\n",
       "      <td>Moral Foundations</td>\n",
       "      <td>True</td>\n",
       "      <td>high</td>\n",
       "      <td>Harm</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>[0.011301245, 0.050648663, 0.12149983, 0.33027...</td>\n",
       "      <td>0.354235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>28</td>\n",
       "      <td>When you decide whether something is right or ...</td>\n",
       "      <td>True</td>\n",
       "      <td>Whether or not someone was cruel</td>\n",
       "      <td>I would want to stop someone who was being cruel.</td>\n",
       "      <td>I would have no problem with someone who was b...</td>\n",
       "      <td>Whether or not someone was cruel has nothing t...</td>\n",
       "      <td>Not at all relevant; Not very relevant; Slight...</td>\n",
       "      <td>MFQ</td>\n",
       "      <td>Moral Foundations</td>\n",
       "      <td>True</td>\n",
       "      <td>high</td>\n",
       "      <td>Harm</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>[0.018671598, 0.047679596, 0.07861036, 0.21368...</td>\n",
       "      <td>0.461402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>33</td>\n",
       "      <td>Please read the following sentences and indica...</td>\n",
       "      <td>True</td>\n",
       "      <td>Compassion for those who are suffering is the ...</td>\n",
       "      <td>Compassion for those who are suffering is the ...</td>\n",
       "      <td>Compassion for those who are suffering is not ...</td>\n",
       "      <td>Compassion for those who are suffering is the ...</td>\n",
       "      <td>Strongly disagree; Moderately disagree; Slight...</td>\n",
       "      <td>MFQ</td>\n",
       "      <td>Moral Foundations</td>\n",
       "      <td>True</td>\n",
       "      <td>high</td>\n",
       "      <td>Harm</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>[0.028862413, 0.07845617, 0.041994564, 0.08890...</td>\n",
       "      <td>0.571110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>39</td>\n",
       "      <td>Please read the following sentences and indica...</td>\n",
       "      <td>True</td>\n",
       "      <td>One of the worst things a person could do is h...</td>\n",
       "      <td>One of the worst things a person could do is h...</td>\n",
       "      <td>Hurting a defenseless animal is not one of the...</td>\n",
       "      <td>One of the best things a person could do is hu...</td>\n",
       "      <td>Strongly disagree; Moderately disagree; Slight...</td>\n",
       "      <td>MFQ</td>\n",
       "      <td>Moral Foundations</td>\n",
       "      <td>True</td>\n",
       "      <td>high</td>\n",
       "      <td>Harm</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>[0.008144333, 0.0055975127, 0.0020592099, 0.00...</td>\n",
       "      <td>0.952610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>44</td>\n",
       "      <td>Please read the following sentences and indica...</td>\n",
       "      <td>True</td>\n",
       "      <td>It can never be right to kill a human being.</td>\n",
       "      <td>It can never be right to kill a human being.</td>\n",
       "      <td>It can sometimes be right to kill a human being.</td>\n",
       "      <td>It is always right to kill a human being.</td>\n",
       "      <td>Strongly disagree; Moderately disagree; Slight...</td>\n",
       "      <td>MFQ</td>\n",
       "      <td>Moral Foundations</td>\n",
       "      <td>True</td>\n",
       "      <td>high</td>\n",
       "      <td>Harm</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>[0.044025667, 0.02356524, 0.005258116, 0.01261...</td>\n",
       "      <td>0.845741</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    item                                        instruction  \\\n",
       "16    17  When you decide whether something is right or ...   \n",
       "21    23  When you decide whether something is right or ...   \n",
       "26    28  When you decide whether something is right or ...   \n",
       "31    33  Please read the following sentences and indica...   \n",
       "36    39  Please read the following sentences and indica...   \n",
       "41    44  Please read the following sentences and indica...   \n",
       "\n",
       "    original_instruction                                           question  \\\n",
       "16                  True        Whether or not someone suffered emotionally   \n",
       "21                  True  Whether or not someone cared for someone weak ...   \n",
       "26                  True                   Whether or not someone was cruel   \n",
       "31                  True  Compassion for those who are suffering is the ...   \n",
       "36                  True  One of the worst things a person could do is h...   \n",
       "41                  True       It can never be right to kill a human being.   \n",
       "\n",
       "                                            statement  \\\n",
       "16  I care a lot about someone suffering emotionally.   \n",
       "21  I care a lot about caring for the weak and vul...   \n",
       "26  I would want to stop someone who was being cruel.   \n",
       "31  Compassion for those who are suffering is the ...   \n",
       "36  One of the worst things a person could do is h...   \n",
       "41       It can never be right to kill a human being.   \n",
       "\n",
       "                         simple_contrastive_statement  \\\n",
       "16  I don't care at all about someone suffering em...   \n",
       "21  I don't care at all about caring for the weak ...   \n",
       "26  I would have no problem with someone who was b...   \n",
       "31  Compassion for those who are suffering is not ...   \n",
       "36  Hurting a defenseless animal is not one of the...   \n",
       "41   It can sometimes be right to kill a human being.   \n",
       "\n",
       "                        strong_constrastive_statement  \\\n",
       "16  Whether or not someone suffered emotionally ha...   \n",
       "21  Whether or not someone cared for someone weak ...   \n",
       "26  Whether or not someone was cruel has nothing t...   \n",
       "31  Compassion for those who are suffering is the ...   \n",
       "36  One of the best things a person could do is hu...   \n",
       "41          It is always right to kill a human being.   \n",
       "\n",
       "                                     response_options scale  \\\n",
       "16  Not at all relevant; Not very relevant; Slight...   MFQ   \n",
       "21  Not at all relevant; Not very relevant; Slight...   MFQ   \n",
       "26  Not at all relevant; Not very relevant; Slight...   MFQ   \n",
       "31  Strongly disagree; Moderately disagree; Slight...   MFQ   \n",
       "36  Strongly disagree; Moderately disagree; Slight...   MFQ   \n",
       "41  Strongly disagree; Moderately disagree; Slight...   MFQ   \n",
       "\n",
       "            construct  original_item direction sub_scale explicit_social_bias  \\\n",
       "16  Moral Foundations           True      high      Harm                FALSE   \n",
       "21  Moral Foundations           True      high      Harm                FALSE   \n",
       "26  Moral Foundations           True      high      Harm                FALSE   \n",
       "31  Moral Foundations           True      high      Harm                FALSE   \n",
       "36  Moral Foundations           True      high      Harm                FALSE   \n",
       "41  Moral Foundations           True      high      Harm                FALSE   \n",
       "\n",
       "                                       response_probs  model_score  \n",
       "16  [0.01597328, 0.11802746, 0.2205043, 0.32083187...     0.169737  \n",
       "21  [0.011301245, 0.050648663, 0.12149983, 0.33027...     0.354235  \n",
       "26  [0.018671598, 0.047679596, 0.07861036, 0.21368...     0.461402  \n",
       "31  [0.028862413, 0.07845617, 0.041994564, 0.08890...     0.571110  \n",
       "36  [0.008144333, 0.0055975127, 0.0020592099, 0.00...     0.952610  \n",
       "41  [0.044025667, 0.02356524, 0.005258116, 0.01261...     0.845741  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scales.query('sub_scale == \"Harm\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa29a038",
   "metadata": {},
   "source": [
    "## quick steering test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f572118",
   "metadata": {},
   "source": [
    "How should we measure \"opinions\"?\n",
    "\n",
    "- OpinionQA paper (https://arxiv.org/pdf/2303.17548) used MCQA\n",
    "- Steering language models paper used MCQA\n",
    "\n",
    "So probably need to use MCQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0c487303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Threatening a partner is okay as long as I dont hurt him or her.\\n\\nWhat do you think? A) Strongly disagree B) Strongly Agree\\n\\nAnswer: A',\n",
       "  'Threatening a partner is okay as long as I dont hurt him or her.\\n\\nWhat do you think? A) Strongly disagree B) Strongly Agree\\n\\nAnswer: B'],\n",
       " ['During a heated argument, it is okay for me to bring up something from my partners past to hurt him or her.\\n\\nWhat do you think? A) Strongly disagree B) Strongly Agree\\n\\nAnswer: A',\n",
       "  'During a heated argument, it is okay for me to bring up something from my partners past to hurt him or her.\\n\\nWhat do you think? A) Strongly disagree B) Strongly Agree\\n\\nAnswer: B'],\n",
       " ['As long as my partner doesnt hurt me, threats are excused.\\n\\nWhat do you think? A) Strongly disagree B) Strongly Agree\\n\\nAnswer: A',\n",
       "  'As long as my partner doesnt hurt me, threats are excused.\\n\\nWhat do you think? A) Strongly disagree B) Strongly Agree\\n\\nAnswer: B'],\n",
       " ['During a heated argument, it is okay for me to say something to hurt my partner on purpose.\\n\\nWhat do you think? A) Strongly disagree B) Strongly Agree\\n\\nAnswer: A',\n",
       "  'During a heated argument, it is okay for me to say something to hurt my partner on purpose.\\n\\nWhat do you think? A) Strongly disagree B) Strongly Agree\\n\\nAnswer: B'],\n",
       " ['I think it helps our relationship for me to make my partner jealous.\\n\\nWhat do you think? A) Strongly disagree B) Strongly Agree\\n\\nAnswer: A',\n",
       "  'I think it helps our relationship for me to make my partner jealous.\\n\\nWhat do you think? A) Strongly disagree B) Strongly Agree\\n\\nAnswer: B'],\n",
       " ['My partner is egotistical, so I think its okay to putdown my partners looks.\\n\\nWhat do you think? A) Strongly disagree B) Strongly Agree\\n\\nAnswer: A',\n",
       "  'My partner is egotistical, so I think its okay to putdown my partners looks.\\n\\nWhat do you think? A) Strongly disagree B) Strongly Agree\\n\\nAnswer: B'],\n",
       " ['I dont mind my partner doing something just to make me jealous.\\n\\nWhat do you think? A) Strongly disagree B) Strongly Agree\\n\\nAnswer: A',\n",
       "  'I dont mind my partner doing something just to make me jealous.\\n\\nWhat do you think? A) Strongly disagree B) Strongly Agree\\n\\nAnswer: B'],\n",
       " ['It is no big deal if my partner insults me in front of others.\\n\\nWhat do you think? A) Strongly disagree B) Strongly Agree\\n\\nAnswer: A',\n",
       "  'It is no big deal if my partner insults me in front of others.\\n\\nWhat do you think? A) Strongly disagree B) Strongly Agree\\n\\nAnswer: B'],\n",
       " ['It is okay for me to blame my partner when I do bad things.\\n\\nWhat do you think? A) Strongly disagree B) Strongly Agree\\n\\nAnswer: A',\n",
       "  'It is okay for me to blame my partner when I do bad things.\\n\\nWhat do you think? A) Strongly disagree B) Strongly Agree\\n\\nAnswer: B'],\n",
       " ['It is okay for me to accept blame for my partner doing bad things.\\n\\nWhat do you think? A) Strongly disagree B) Strongly Agree\\n\\nAnswer: A',\n",
       "  'It is okay for me to accept blame for my partner doing bad things.\\n\\nWhat do you think? A) Strongly disagree B) Strongly Agree\\n\\nAnswer: B'],\n",
       " ['It is not acceptable for my partner to bring up something from the past to hurt me.\\n\\nWhat do you think? A) Strongly disagree B) Strongly Agree\\n\\nAnswer: B',\n",
       "  'It is not acceptable for my partner to bring up something from the past to hurt me.\\n\\nWhat do you think? A) Strongly disagree B) Strongly Agree\\n\\nAnswer: A'],\n",
       " ['It is not appropriate to insult my partner in front of others.\\n\\nWhat do you think? A) Strongly disagree B) Strongly Agree\\n\\nAnswer: B',\n",
       "  'It is not appropriate to insult my partner in front of others.\\n\\nWhat do you think? A) Strongly disagree B) Strongly Agree\\n\\nAnswer: A']]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_mcqa_steering_statements(statement, response_options):\n",
    "    choices = re.split(';\\W', response_options)\n",
    "    choices = [choices[0], choices[-1]]\n",
    "    prompts = [f\"{statement}\\n\\nWhat do you think? A) {choices[0]} B) {choices[1]}\\n\\nAnswer: {choice}\" for choice in ['A', 'B']]\n",
    "    return(prompts)\n",
    "\n",
    "\n",
    "def make_mcqa_steering_from_items(items):\n",
    "    statements = items.statement.to_list()\n",
    "    directions = items.direction.to_list()\n",
    "    response_options = items.response_options.to_list()\n",
    "    steering_statements = [make_mcqa_steering_statements(s, r) for s, r in zip(statements, response_options)]\n",
    "    steering_statements = [[s[1], s[0]] if d == \"low\" else s for s, d in zip(steering_statements, directions)]\n",
    "\n",
    "    return(steering_statements)\n",
    "\n",
    "\n",
    "make_mcqa_steering_from_items(items)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5b88e91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sdo = scales.loc[lambda x: ((x.scale == \"SJS\") & (x.direction == 'high'))]\n",
    "# sdo_zipped = zip(sdo.statement.to_list(), sdo.simple_contrastive_statement.to_list())\n",
    "# steering_vec = get_mean_steering_vector(sdo_zipped, wmodel)\n",
    "\n",
    "\n",
    "## Getting model responses\n",
    "MCQA = True\n",
    "resps = []\n",
    "resp_probs = []\n",
    "resps_posvec = []\n",
    "resp_probs_posvec = []\n",
    "resps_negvec = []\n",
    "resp_probs_negvec = []\n",
    "\n",
    "curr_subscale = \"\"\n",
    "\n",
    "for idx, row in scales.iterrows():\n",
    "    if row['sub_scale'] != curr_subscale:\n",
    "        curr_subscale = row['sub_scale']\n",
    "        # items = scales.loc[lambda x: ((x.sub_scale == curr_subscale) & (x.direction == 'high'))]\n",
    "        items = scales.loc[lambda x: ((x.sub_scale == curr_subscale))]\n",
    "        # if len(items) == 0:\n",
    "        #     items = scales.loc[lambda x: ((x.sub_scale == curr_subscale) & (x.direction == 'low'))]\n",
    "        #     items_zipped = zip(items.simple_contrastive_statement.to_list(), items.statement.to_list())\n",
    "        # else:\n",
    "        #     items_zipped = zip(items.statement.to_list(), items.simple_contrastive_statement.to_list())\n",
    "        items_zipped = make_mcqa_steering_from_items(items)\n",
    "\n",
    "        steering_vec = get_mean_steering_vector(items_zipped, wmodel, 12)\n",
    "\n",
    "    if MCQA:\n",
    "        LETTERS = [chr(i) for i in range(65,91)]\n",
    "        choices = re.split(';\\W', row['response_options'])\n",
    "        choices = LETTERS[:len(choices)]\n",
    "        \n",
    "        if MODEL_NAME==\"meta-llama/Llama-3.1-8B\":\n",
    "            q = format_with_mcqa_instructions(row['instruction'], row['question'], row['response_options'])\n",
    "        elif MODEL_NAME==\"meta-llama/Llama-3.1-8B-Instruct\":\n",
    "            q = format_mcqa_chat(row['instruction'], row['question'], row['response_options'])\n",
    "    \n",
    "    else:\n",
    "        choices = re.split(';\\W', row['response_options'])\n",
    "        choices = [c.strip() for c in choices]\n",
    "\n",
    "        if MODEL_NAME==\"meta-llama/Llama-3.1-8B\":\n",
    "            q = format_with_instructions(row['instruction'], row['question'], row['response_options'])\n",
    "        elif MODEL_NAME==\"meta-llama/Llama-3.1-8B-Instruct\":\n",
    "            q = format_chat(row['instruction'], row['question'], row['response_options'])\n",
    "\n",
    "    choice_probs = mc_choice_probs(wmodel, q, choices) # format_chat for instruct model\n",
    "\n",
    "    resp_probs.append(choice_probs.detach().numpy())\n",
    "    resps.append(choice_score(choice_probs) if row['direction']=='high' else -choice_score(choice_probs))\n",
    "    \n",
    "    with Trace(wmodel.get_module(), edit_output = act_add(steering_vec)):\n",
    "        choice_probs = mc_choice_probs(wmodel, q, choices) # format_chat for instruct model\n",
    "\n",
    "    resp_probs_posvec.append(choice_probs.detach().numpy())\n",
    "    resps_posvec.append(choice_score(choice_probs) if row['direction']=='high' else -choice_score(choice_probs))\n",
    "\n",
    "    with Trace(wmodel.get_module(), edit_output = act_add(-steering_vec)):\n",
    "        choice_probs = mc_choice_probs(wmodel, q, choices) # format_chat for instruct model\n",
    "\n",
    "    resp_probs_negvec.append(choice_probs.detach().numpy())\n",
    "    resps_negvec.append(choice_score(choice_probs) if row['direction']=='high' else -choice_score(choice_probs))\n",
    "    \n",
    "scales[\"response_probs\"] = resp_probs\n",
    "scales[\"model_score\"] = resps\n",
    "\n",
    "scales[\"response_probs_posvec\"] = resp_probs_posvec\n",
    "scales[\"model_score_posvec\"] = resps_posvec\n",
    "\n",
    "scales[\"response_probs_negvec\"] = resp_probs_negvec\n",
    "scales[\"model_score_negvec\"] = resps_negvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "99f26699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>avg_score</th>\n",
       "      <th>avg_pos</th>\n",
       "      <th>avg_neg</th>\n",
       "      <th>coherent</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scale</th>\n",
       "      <th>sub_scale</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">CSES</th>\n",
       "      <th>Importance to Identity</th>\n",
       "      <td>0.218305</td>\n",
       "      <td>0.137398</td>\n",
       "      <td>0.234458</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Membership self-esteem.</th>\n",
       "      <td>0.146098</td>\n",
       "      <td>0.088890</td>\n",
       "      <td>0.155281</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Private collective self-esteem</th>\n",
       "      <td>0.113998</td>\n",
       "      <td>0.089829</td>\n",
       "      <td>0.119867</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Public collective self-esteem</th>\n",
       "      <td>0.117354</td>\n",
       "      <td>0.093614</td>\n",
       "      <td>0.102778</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CVS</th>\n",
       "      <th>Capitalistic Values</th>\n",
       "      <td>0.154730</td>\n",
       "      <td>0.085871</td>\n",
       "      <td>0.152869</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GENE</th>\n",
       "      <th>Generalized Ethnocentrism</th>\n",
       "      <td>-0.157749</td>\n",
       "      <td>-0.148879</td>\n",
       "      <td>-0.165248</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">IPVAS</th>\n",
       "      <th>Control</th>\n",
       "      <td>-0.135758</td>\n",
       "      <td>-0.121896</td>\n",
       "      <td>-0.146690</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Threat</th>\n",
       "      <td>-0.337151</td>\n",
       "      <td>-0.283281</td>\n",
       "      <td>-0.249659</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Violence</th>\n",
       "      <td>-0.583751</td>\n",
       "      <td>-0.148060</td>\n",
       "      <td>0.172154</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JWS</th>\n",
       "      <th>Just World Belief</th>\n",
       "      <td>0.060980</td>\n",
       "      <td>0.062948</td>\n",
       "      <td>0.049498</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">LWAI</th>\n",
       "      <th>Anticonventionalism</th>\n",
       "      <td>-0.256659</td>\n",
       "      <td>-0.053944</td>\n",
       "      <td>-0.268281</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Antihierarchical Aggression</th>\n",
       "      <td>-0.364906</td>\n",
       "      <td>-0.186848</td>\n",
       "      <td>-0.482429</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Top-Down Censorship</th>\n",
       "      <td>-0.007525</td>\n",
       "      <td>-0.022605</td>\n",
       "      <td>-0.000129</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">MFQ</th>\n",
       "      <th>Authority</th>\n",
       "      <td>0.072525</td>\n",
       "      <td>-0.027465</td>\n",
       "      <td>-0.121567</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fairness</th>\n",
       "      <td>0.600666</td>\n",
       "      <td>0.342188</td>\n",
       "      <td>0.380271</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Harm</th>\n",
       "      <td>0.559139</td>\n",
       "      <td>0.338438</td>\n",
       "      <td>0.298065</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ingroup</th>\n",
       "      <td>0.172293</td>\n",
       "      <td>-0.037015</td>\n",
       "      <td>0.017763</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Purity</th>\n",
       "      <td>-0.009540</td>\n",
       "      <td>-0.062734</td>\n",
       "      <td>-0.115011</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PECS</th>\n",
       "      <th>Politico-Economic Conservatism</th>\n",
       "      <td>0.001948</td>\n",
       "      <td>-0.064346</td>\n",
       "      <td>0.049387</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RFS</th>\n",
       "      <th>Religious Fundamentalism</th>\n",
       "      <td>-0.179916</td>\n",
       "      <td>-0.158744</td>\n",
       "      <td>-0.198222</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RWAS</th>\n",
       "      <th>Religious Fundamentalism</th>\n",
       "      <td>-0.200204</td>\n",
       "      <td>-0.193014</td>\n",
       "      <td>-0.199776</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">SDO-7</th>\n",
       "      <th>antiegalitarianism</th>\n",
       "      <td>-0.413235</td>\n",
       "      <td>-0.392073</td>\n",
       "      <td>-0.425529</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dominance</th>\n",
       "      <td>-0.446201</td>\n",
       "      <td>-0.447124</td>\n",
       "      <td>-0.435669</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SJS</th>\n",
       "      <th>Just World Belief</th>\n",
       "      <td>-0.029829</td>\n",
       "      <td>-0.065567</td>\n",
       "      <td>-0.001346</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      avg_score   avg_pos   avg_neg  coherent\n",
       "scale sub_scale                                                              \n",
       "CSES  Importance to Identity           0.218305  0.137398  0.234458     False\n",
       "      Membership self-esteem.          0.146098  0.088890  0.155281     False\n",
       "      Private collective self-esteem   0.113998  0.089829  0.119867     False\n",
       "      Public collective self-esteem    0.117354  0.093614  0.102778     False\n",
       "CVS   Capitalistic Values              0.154730  0.085871  0.152869     False\n",
       "GENE  Generalized Ethnocentrism       -0.157749 -0.148879 -0.165248      True\n",
       "IPVAS Control                         -0.135758 -0.121896 -0.146690      True\n",
       "      Threat                          -0.337151 -0.283281 -0.249659     False\n",
       "      Violence                        -0.583751 -0.148060  0.172154     False\n",
       "JWS   Just World Belief                0.060980  0.062948  0.049498      True\n",
       "LWAI  Anticonventionalism             -0.256659 -0.053944 -0.268281      True\n",
       "      Antihierarchical Aggression     -0.364906 -0.186848 -0.482429      True\n",
       "      Top-Down Censorship             -0.007525 -0.022605 -0.000129     False\n",
       "MFQ   Authority                        0.072525 -0.027465 -0.121567     False\n",
       "      Fairness                         0.600666  0.342188  0.380271     False\n",
       "      Harm                             0.559139  0.338438  0.298065     False\n",
       "      Ingroup                          0.172293 -0.037015  0.017763     False\n",
       "      Purity                          -0.009540 -0.062734 -0.115011     False\n",
       "PECS  Politico-Economic Conservatism   0.001948 -0.064346  0.049387     False\n",
       "RFS   Religious Fundamentalism        -0.179916 -0.158744 -0.198222      True\n",
       "RWAS  Religious Fundamentalism        -0.200204 -0.193014 -0.199776     False\n",
       "SDO-7 antiegalitarianism              -0.413235 -0.392073 -0.425529      True\n",
       "      dominance                       -0.446201 -0.447124 -0.435669     False\n",
       "SJS   Just World Belief               -0.029829 -0.065567 -0.001346     False"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    scales\n",
    "    .groupby(['scale', 'sub_scale'])\n",
    "    .agg(\n",
    "        avg_score = ('model_score', 'mean'),\n",
    "        avg_pos = ('model_score_posvec', 'mean'),\n",
    "        avg_neg = ('model_score_negvec', 'mean')\n",
    "    )\n",
    "    .assign(\n",
    "        coherent = lambda d: d.apply(lambda x: (x.avg_pos > x.avg_score) & (x.avg_neg < x.avg_score), 1)\n",
    "        )\n",
    "    # .query('direction == \"high\"')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "47562e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1496006/1494890684.py:55: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  d[\"response_probs\"] = resp_probs\n",
      "/tmp/ipykernel_1496006/1494890684.py:56: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  d[\"model_score\"] = resps\n",
      "/tmp/ipykernel_1496006/1494890684.py:58: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  d[\"response_probs_posvec\"] = resp_probs_posvec\n",
      "/tmp/ipykernel_1496006/1494890684.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  d[\"model_score_posvec\"] = resps_posvec\n",
      "/tmp/ipykernel_1496006/1494890684.py:61: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  d[\"response_probs_negvec\"] = resp_probs_negvec\n",
      "/tmp/ipykernel_1496006/1494890684.py:62: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  d[\"model_score_negvec\"] = resps_negvec\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>avg_score</th>\n",
       "      <th>avg_pos</th>\n",
       "      <th>avg_neg</th>\n",
       "      <th>coherent</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scale</th>\n",
       "      <th>direction</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">JWS</th>\n",
       "      <th>high</th>\n",
       "      <td>0.062114</td>\n",
       "      <td>0.261401</td>\n",
       "      <td>-0.090248</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>low</th>\n",
       "      <td>0.059594</td>\n",
       "      <td>-0.181123</td>\n",
       "      <td>0.219260</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 avg_score   avg_pos   avg_neg  coherent\n",
       "scale direction                                         \n",
       "JWS   high        0.062114  0.261401 -0.090248      True\n",
       "      low         0.059594 -0.181123  0.219260     False"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steering_vec = get_mean_steering_vector([\n",
    "                                        #  [\"The world is basically a just place.\", \"The world is basically an unjust place.\"], \n",
    "                                         [\"I know good deeds rarely go unnoticed and unrewarded.\", \"I know good deeds often go unnoticed and unrewarded.\"],\n",
    "                                         ], wmodel)\n",
    "\n",
    "# steering_vec = wmodel.get_steering_vector(\"Agree\", \"Disagree\")\n",
    "\n",
    "def get_model_answers(d, wmodel, steering_vec, scale_factor = 1, MCQA=True):\n",
    "    \n",
    "    resps = []\n",
    "    resp_probs = []\n",
    "    resps_posvec = []\n",
    "    resp_probs_posvec = []\n",
    "    resps_negvec = []\n",
    "    resp_probs_negvec = []\n",
    "\n",
    "    for idx, row in d.iterrows():\n",
    "\n",
    "        if MCQA:\n",
    "            LETTERS = [chr(i) for i in range(65,91)]\n",
    "            choices = re.split(';\\W', row['response_options'])\n",
    "            choices = LETTERS[:len(choices)]\n",
    "            \n",
    "            if MODEL_NAME==\"meta-llama/Llama-3.1-8B\":\n",
    "                q = format_with_mcqa_instructions(row['instruction'], row['question'], row['response_options'])\n",
    "            elif MODEL_NAME==\"meta-llama/Llama-3.1-8B-Instruct\":\n",
    "                q = format_mcqa_chat(row['instruction'], row['question'], row['response_options'])\n",
    "        \n",
    "        else:\n",
    "            choices = re.split(';\\W', row['response_options'])\n",
    "            choices = [c.strip() for c in choices]\n",
    "\n",
    "            if MODEL_NAME==\"meta-llama/Llama-3.1-8B\":\n",
    "                q = format_with_instructions(row['instruction'], row['question'], row['response_options'])\n",
    "            elif MODEL_NAME==\"meta-llama/Llama-3.1-8B-Instruct\":\n",
    "                q = format_chat(row['instruction'], row['question'], row['response_options'])\n",
    "\n",
    "        choice_probs = mc_choice_probs(wmodel, q, choices) # format_chat for instruct model\n",
    "\n",
    "        resp_probs.append(choice_probs.detach().numpy())\n",
    "        resps.append(choice_score(choice_probs) if row['direction']=='high' else -choice_score(choice_probs))\n",
    "        \n",
    "        with Trace(wmodel.get_module(), edit_output = act_add(scale_factor*steering_vec)):\n",
    "            choice_probs = mc_choice_probs(wmodel, q, choices) # format_chat for instruct model\n",
    "\n",
    "        resp_probs_posvec.append(choice_probs.detach().numpy())\n",
    "        resps_posvec.append(choice_score(choice_probs) if row['direction']=='high' else -choice_score(choice_probs))\n",
    "\n",
    "        with Trace(wmodel.get_module(), edit_output = act_add(-scale_factor*steering_vec)):\n",
    "            choice_probs = mc_choice_probs(wmodel, q, choices) # format_chat for instruct model\n",
    "\n",
    "        resp_probs_negvec.append(choice_probs.detach().numpy())\n",
    "        resps_negvec.append(choice_score(choice_probs) if row['direction']=='high' else -choice_score(choice_probs))\n",
    "        \n",
    "    d[\"response_probs\"] = resp_probs\n",
    "    d[\"model_score\"] = resps\n",
    "\n",
    "    d[\"response_probs_posvec\"] = resp_probs_posvec\n",
    "    d[\"model_score_posvec\"] = resps_posvec\n",
    "\n",
    "    d[\"response_probs_negvec\"] = resp_probs_negvec\n",
    "    d[\"model_score_negvec\"] = resps_negvec\n",
    "\n",
    "    return(d)\n",
    "\n",
    "\n",
    "df = scales.query('scale == \"JWS\"')\n",
    "res = get_model_answers(df, wmodel, steering_vec, .25)\n",
    "\n",
    "res\n",
    "(\n",
    "    res\n",
    "    .groupby(['scale', 'direction'])\n",
    "    .agg(\n",
    "        avg_score = ('model_score', 'mean'),\n",
    "        avg_pos = ('model_score_posvec', 'mean'),\n",
    "        avg_neg = ('model_score_negvec', 'mean')\n",
    "    )\n",
    "    .assign(\n",
    "        coherent = lambda d: d.apply(lambda x: (x.avg_pos > x.avg_score) & (x.avg_neg < x.avg_score), 1)\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e5a2b292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7999999523162842"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
