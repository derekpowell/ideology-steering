{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2724c7b6-3913-4e88-b25b-94e840895c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/dmpowell/.cache/huggingface\n",
      "/scratch/dmpowell/.cache/huggingface/datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmpowell/.conda/envs/py3.11transformers4.44/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device =  cuda\n"
     ]
    }
   ],
   "source": [
    "## ---------------------------------------------------------------------\n",
    "## set up configs for huggingface hub and OS paths on HPC cluster -- make sure config.ini is correct\n",
    "## ---------------------------------------------------------------------\n",
    "import configparser\n",
    "\n",
    "def scratch_path():\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(\"config.ini\")\n",
    "    return \"/scratch/\" + config[\"user\"][\"username\"]\n",
    "\n",
    "import os\n",
    "if os.path.isdir(scratch_path()):\n",
    "    os.environ['TRANSFORMERS_CACHE'] = scratch_path() + '/.cache/huggingface'\n",
    "    os.environ['HF_DATASETS_CACHE'] = scratch_path() + '/.cache/huggingface/datasets'\n",
    "print(os.getenv('TRANSFORMERS_CACHE'))\n",
    "print(os.getenv('HF_DATASETS_CACHE'))\n",
    "\n",
    "## ---------------------------------------------------------------------\n",
    "## Load libraries\n",
    "## ---------------------------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel, LlamaForCausalLM, LlamaTokenizer, AutoModelForCausalLM\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from baukit import Trace\n",
    "\n",
    "from steering import *\n",
    "## ---------------------------------------------------------------------\n",
    "## Ensure GPU is available -- device should == 'cuda'\n",
    "## ---------------------------------------------------------------------\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device = \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de83defa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b06d2e76125457bb62049cbf76914cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41ac03e801d840c4847341d2f688c79c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_NAME = \"meta-llama/Llama-3.1-8B\"\n",
    "\n",
    "wmodel = SteeringModel(\n",
    "    AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,  # Replace this with the 70B variant if available\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=device  # Automatically distributes the model across available GPUs\n",
    "    ),\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, device = 'cuda', use_fast = False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5cb158",
   "metadata": {},
   "source": [
    "## Multiple choice\n",
    "\n",
    "Here is a basic implementation of multiple choice answering using \"cloze\" probabilities. This should roughly work with both raw and instruction-tuned models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "473a7440",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def answer_choice_list(choices):\n",
    "    options = re.split(r'\\s*\\(\\w\\)\\s*', choices)\n",
    "    return( [option.strip() for option in options if option] )\n",
    "\n",
    "\n",
    "def format_question(question):\n",
    "    return f\"Q: {question}\\nA:\"\n",
    "\n",
    "\n",
    "def format_statement(question):\n",
    "    return f\"Please rate your agreement with the following statement.Statement: {question}\\nResponse:\"\n",
    "\n",
    "\n",
    "def mc_choice_probs(model, question, choices, pad = True):\n",
    "    prompt = question\n",
    "    if pad:\n",
    "        choices = [\" \" + c for c in choices] # pad all the \n",
    "    \n",
    "    prompts = [prompt for c in choices]\n",
    "    \n",
    "    logits = torch.tensor([model.completion_logprob(x[0], x[1]) for x in zip(prompts, choices)])\n",
    "    \n",
    "    return(F.log_softmax(logits, -1).exp())\n",
    "\n",
    "\n",
    "def choice_score(choice_probs):\n",
    "    # calculate score on -1 to 1 scale\n",
    "    choice_score01 = choice_probs @ torch.arange(len(choice_probs), dtype = choice_probs.dtype)/(len(choice_probs)-1)\n",
    "    return (choice_score01.item() - .5)*2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7806dfd9",
   "metadata": {},
   "source": [
    "For any agree/disagree etc. style scales, we can take the choice probabilities and compute a \"score\". I noticed the model seems to have a really strong \"agree\" bias when we have a pure \"agree\" option. Will need to look into this, probably some literature on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2ecca48e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.6165903210639954,\n",
       " tensor([8.0756e-01, 7.1667e-04, 1.2991e-04, 5.1218e-04, 1.9108e-01]))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choice_probs = mc_choice_probs(wmodel, format_question('Slavery benefitted the slaves, many of whom learned valuable skills.'), ['Strongly disagree', 'Somewhat disagree', \"Neither agree nor disagree\", 'Somewhat agree', 'Strongly agree'])\n",
    "choice_score(choice_probs), choice_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0229b7e",
   "metadata": {},
   "source": [
    "## Steering\n",
    "\n",
    "Applying a steering vector shifts generations ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99a54164",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In my opinion, dogs are the best animals. They are loyal, protective, and always happy to see you. I have']\n",
      "--------------------\n",
      "['In my opinion, dogs are one of the best things that ever happened to me. I have been a dog owner for most']\n",
      "--------------------\n",
      "['In my opinion, dogs are the most disgusting animals in the world. They are the most filthy animals on the face of the']\n"
     ]
    }
   ],
   "source": [
    "def get_mean_steering_vector(ziplist, model):\n",
    "    return torch.concat([model.get_steering_vector(s1, s2) for s1, s2 in ziplist]).mean(0).unsqueeze(0)\n",
    "\n",
    "\n",
    "def act_add(steering_vec):\n",
    "    def act_add(output):\n",
    "        z = torch.ones_like(output[0])\n",
    "        z = z * steering_vec\n",
    "        out = output[0] + z\n",
    "        return (out, output[1])\n",
    "    return act_add\n",
    "\n",
    "\n",
    "steering_vec = get_mean_steering_vector([[\"Love\", \"Hate\"], [\"Beautiful\", \"Disgusting\"], [\"Loving\", \"Hateful\"]], wmodel)\n",
    "x\n",
    "test_sentence = \"In my opinion, dogs\"\n",
    "\n",
    "print(wmodel.generate(test_sentence, max_new_tokens = 20))\n",
    "print(\"-\"*20)\n",
    "with Trace(wmodel.get_module(), edit_output = act_add(steering_vec)):\n",
    "    print(wmodel.generate(test_sentence, max_new_tokens = 20))\n",
    "print(\"-\"*20)\n",
    "with Trace(wmodel.get_module(), edit_output = act_add(-steering_vec)):\n",
    "    print(wmodel.generate(test_sentence, max_new_tokens = 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc77d7fe",
   "metadata": {},
   "source": [
    "### \"survey\" responses and attitude scores\n",
    "\n",
    "Can apply the same thing to our multiple choice decisions and calculated scores. Adding a relevant steering vector affects the responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3bd318be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46433234214782715 tensor([2.0559e-01, 3.6450e-02, 2.1698e-04, 1.3918e-01, 6.1856e-01])\n",
      "-0.6752313077449799 tensor([8.3680e-01, 2.4354e-04, 2.1572e-04, 2.0873e-03, 1.6065e-01])\n"
     ]
    }
   ],
   "source": [
    "steering_vec = wmodel.get_steering_vector(\"Slavery is a proud part of American history.\", \"Slavery is the most shameful part of America's past.\")\n",
    "\n",
    "# steering_vec = get_mean_steering_vector([[\"Some races are inferior to others.\", \"No race is inferior to others.\"], [\"White people are superior to black people.\", \"White and black people are equally good.\"]], wmodel)\n",
    "\n",
    "with Trace(wmodel.get_module(), edit_output = act_add(2*steering_vec)):\n",
    "    # print(wmodel.generate(test_sentence, max_new_tokens = 20))\n",
    "    choice_probs = mc_choice_probs(wmodel, format_question('Slavery benefitted the slaves who learned valuable skills.'), ['Strongly disagree',  'Somewhat disagree', \"Neither agree nor disagree\", 'Somewhat agree', 'Strongly agree'])\n",
    "    print(choice_score(choice_probs), choice_probs) \n",
    "\n",
    "with Trace(wmodel.get_module(), edit_output = act_add(-2*steering_vec)):\n",
    "    # print(wmodel.generate(test_sentence, max_new_tokens = 20))\n",
    "    choice_probs = mc_choice_probs(wmodel, format_question('Slavery benefitted the slaves who learned valuable skills.'), ['Strongly disagree',  'Somewhat disagree', \"Neither agree nor disagree\", 'Somewhat agree', 'Strongly agree'])\n",
    "    print(choice_score(choice_probs), choice_probs) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed5efa7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
