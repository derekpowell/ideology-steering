{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2724c7b6-3913-4e88-b25b-94e840895c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/dmpowell/.cache/huggingface\n",
      "/scratch/dmpowell/.cache/huggingface/datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmpowell/.conda/envs/py3.11transformers4.44/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device =  cuda\n"
     ]
    }
   ],
   "source": [
    "## ---------------------------------------------------------------------\n",
    "## set up configs for huggingface hub and OS paths on HPC cluster -- make sure config.ini is correct\n",
    "## ---------------------------------------------------------------------\n",
    "import configparser\n",
    "\n",
    "def scratch_path():\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(\"config.ini\")\n",
    "    return \"/scratch/\" + config[\"user\"][\"username\"]\n",
    "\n",
    "import os\n",
    "if os.path.isdir(scratch_path()):\n",
    "    os.environ['TRANSFORMERS_CACHE'] = scratch_path() + '/.cache/huggingface'\n",
    "    os.environ['HF_DATASETS_CACHE'] = scratch_path() + '/.cache/huggingface/datasets'\n",
    "print(os.getenv('TRANSFORMERS_CACHE'))\n",
    "print(os.getenv('HF_DATASETS_CACHE'))\n",
    "\n",
    "## ---------------------------------------------------------------------\n",
    "## Load libraries\n",
    "## ---------------------------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel, LlamaForCausalLM, LlamaTokenizer, AutoModelForCausalLM\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from baukit import Trace\n",
    "\n",
    "from steering import *\n",
    "## ---------------------------------------------------------------------\n",
    "## Ensure GPU is available -- device should == 'cuda'\n",
    "## ---------------------------------------------------------------------\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device = \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "de83defa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b3103a6a4c14d8badb55835d7c8950e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14659c607d4c4827a34fe0f6c6e7c75c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "# MODEL_NAME = \"meta-llama/Llama-3.1-8B\"\n",
    "\n",
    "wmodel = SteeringModel(\n",
    "    AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,  # Replace this with the 70B variant if available\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=device  # Automatically distributes the model across available GPUs\n",
    "    ),\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, device = 'cuda', use_fast = False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5cb158",
   "metadata": {},
   "source": [
    "## Multiple choice\n",
    "\n",
    "Here is a basic implementation of multiple choice answering using \"cloze\" probabilities. This should roughly work with both raw and instruction-tuned models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "473a7440",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def answer_choice_list(choices):\n",
    "    options = re.split(r'\\s*\\(\\w\\)\\s*', choices)\n",
    "    return( [option.strip() for option in options if option] )\n",
    "\n",
    "\n",
    "def format_question(question):\n",
    "    return f\"Q: {question}\\nA:\"\n",
    "\n",
    "\n",
    "def format_statement(question):\n",
    "    return f\"Please rate your agreement with the following statement. Statement: {question}\\nResponse:\"\n",
    "\n",
    "\n",
    "def format_chat_question(question):\n",
    "    return f\"Please rate your agreement with the following statement. Statement: {question}\"\n",
    "\n",
    "\n",
    "def format_chat(question):\n",
    "\n",
    "    chat = [\n",
    "        {\"role\": \"user\", \"content\": format_chat_question(row['original_statement'])},\n",
    "        {\"role\": \"system\", \"content\": \"My Response:\"}\n",
    "    ]\n",
    "\n",
    "    tokens = wmodel.tok.apply_chat_template(chat, tokenize=True, continue_final_message=True)[:-1]\n",
    "\n",
    "    return(wmodel.tok.decode(tokens))\n",
    "\n",
    "\n",
    "def mc_choice_probs(model, question, choices, pad = True):\n",
    "    prompt = question\n",
    "    if pad:\n",
    "        choices = [\" \" + c for c in choices] # pad all the \n",
    "    \n",
    "    prompts = [prompt for c in choices]\n",
    "    \n",
    "    logits = torch.tensor([model.completion_logprob(x[0], x[1]) for x in zip(prompts, choices)])\n",
    "    \n",
    "    return(F.log_softmax(logits, -1).exp())\n",
    "\n",
    "\n",
    "def choice_score(choice_probs):\n",
    "    # calculate score on -1 to 1 scale\n",
    "    choice_score01 = choice_probs @ torch.arange(len(choice_probs), dtype = choice_probs.dtype)/(len(choice_probs)-1)\n",
    "    return (choice_score01.item() - .5)*2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7806dfd9",
   "metadata": {},
   "source": [
    "For any agree/disagree etc. style scales, we can take the choice probabilities and compute a \"score\". I noticed the model seems to have a really strong \"agree\" bias when we have a pure \"agree\" option. Will need to look into this, probably some literature on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ecca48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-0.8667596280574799, tensor([0.8655, 0.0718, 0.0178, 0.0207, 0.0242]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choice_probs = mc_choice_probs(wmodel, format_question('Slavery benefitted the slaves, many of whom learned valuable skills.'), ['Strongly disagree', 'Somewhat disagree', \"Neither agree nor disagree\", 'Somewhat agree', 'Strongly agree'])\n",
    "choice_score(choice_probs), choice_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0229b7e",
   "metadata": {},
   "source": [
    "## Steering\n",
    "\n",
    "Applying a steering vector shifts generations ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "99a54164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In my opinion, dogs are one of the most loving and loyal animals on this planet. They bring so much joy and companions']\n",
      "--------------------\n",
      "['In my opinion, dogs are the best companions you can have. They are loyal, gentle, and loving. They are also']\n",
      "--------------------\n",
      "['In my opinion, dogs are the worst. They are everywhere, everywhere, everywhere. On the streets, on public transportation,']\n"
     ]
    }
   ],
   "source": [
    "def get_mean_steering_vector(ziplist, model):\n",
    "    return torch.concat([model.get_steering_vector(s1, s2) for s1, s2 in ziplist]).mean(0).unsqueeze(0)\n",
    "\n",
    "\n",
    "def act_add(steering_vec):\n",
    "    def act_add(output):\n",
    "        z = torch.ones_like(output[0])\n",
    "        z = z * steering_vec\n",
    "        out = output[0] + z\n",
    "        return (out, output[1])\n",
    "    return act_add\n",
    "\n",
    "\n",
    "steering_vec = get_mean_steering_vector([[\"Love\", \"Hate\"], [\"Beautiful\", \"Disgusting\"], [\"Loving\", \"Hateful\"]], wmodel)\n",
    "\n",
    "test_sentence = \"In my opinion, dogs\"\n",
    "\n",
    "print(wmodel.generate(test_sentence, max_new_tokens = 20))\n",
    "print(\"-\"*20)\n",
    "with Trace(wmodel.get_module(), edit_output = act_add(steering_vec)):\n",
    "    print(wmodel.generate(test_sentence, max_new_tokens = 20))\n",
    "print(\"-\"*20)\n",
    "with Trace(wmodel.get_module(), edit_output = act_add(-1.5*steering_vec)):\n",
    "    print(wmodel.generate(test_sentence, max_new_tokens = 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc77d7fe",
   "metadata": {},
   "source": [
    "### \"survey\" responses and attitude scores\n",
    "\n",
    "Can apply the same thing to our multiple choice decisions and calculated scores. Adding a relevant steering vector affects the responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bd318be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.6855780482292175 tensor([0.8168, 0.0296, 0.0062, 0.0029, 0.1445])\n",
      "-0.9813397862017155 tensor([9.8887e-01, 1.2226e-03, 4.3364e-04, 2.6493e-03, 6.8207e-03])\n"
     ]
    }
   ],
   "source": [
    "steering_vec = wmodel.get_steering_vector(\"Slavery is a proud part of American history.\", \"Slavery is the most shameful part of America's past.\")\n",
    "\n",
    "# steering_vec = get_mean_steering_vector([[\"Some races are inferior to others.\", \"No race is inferior to others.\"], [\"White people are superior to black people.\", \"White and black people are equally good.\"]], wmodel)\n",
    "\n",
    "with Trace(wmodel.get_module(), edit_output = act_add(steering_vec)):\n",
    "    # print(wmodel.generate(test_sentence, max_new_tokens = 20))\n",
    "    choice_probs = mc_choice_probs(wmodel, format_question('Slavery benefitted the slaves who learned valuable skills.'), ['Strongly disagree',  'Somewhat disagree', \"Neither agree nor disagree\", 'Somewhat agree', 'Strongly agree'])\n",
    "    print(choice_score(choice_probs), choice_probs) \n",
    "\n",
    "with Trace(wmodel.get_module(), edit_output = act_add(-steering_vec)):\n",
    "    # print(wmodel.generate(test_sentence, max_new_tokens = 20))\n",
    "    choice_probs = mc_choice_probs(wmodel, format_question('Slavery benefitted the slaves who learned valuable skills.'), ['Strongly disagree',  'Somewhat disagree', \"Neither agree nor disagree\", 'Somewhat agree', 'Strongly agree'])\n",
    "    print(choice_score(choice_probs), choice_probs) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed5efa7",
   "metadata": {},
   "source": [
    "## Applying to survey ideology scales\n",
    "\n",
    "First, to generate the model's answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "32ecfb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "scales = pd.read_csv(\"data/scales.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8cdb383c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting model responses\n",
    "resps = []\n",
    "resp_probs = []\n",
    "\n",
    "for idx, row in scales.iterrows():\n",
    "    choices = re.split(';\\W', row['response'])\n",
    "    choices = [c.strip() for c in choices]\n",
    "    choice_probs = mc_choice_probs(wmodel, format_chat(row['original_statement']), choices) # format_chat for instruct model\n",
    "\n",
    "    resp_probs.append(choice_probs.detach().numpy())\n",
    "    resps.append(choice_score(choice_probs) if row['direction']=='high' else -choice_score(choice_probs))\n",
    "    \n",
    "scales[\"response_probs\"] = resp_probs\n",
    "scales[\"model_score\"] = resps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "29e5646f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>avg_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scale</th>\n",
       "      <th>sub_scale</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">CSES</th>\n",
       "      <th>Importance to Identity</th>\n",
       "      <td>0.003708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Membership self-esteem.</th>\n",
       "      <td>0.011190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Private collective self-esteem</th>\n",
       "      <td>0.016076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Public collective self-esteem</th>\n",
       "      <td>0.014440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">IPVAS</th>\n",
       "      <th>Control</th>\n",
       "      <td>-0.633278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Threat</th>\n",
       "      <td>-0.333069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Violence</th>\n",
       "      <td>-0.999981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">LWAI</th>\n",
       "      <th>Anticonventionalism</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Antihierarchical Aggression</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Top-Down Censorship</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">MFQ</th>\n",
       "      <th>Authority</th>\n",
       "      <td>-0.017626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fairness</th>\n",
       "      <td>0.081313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Harm</th>\n",
       "      <td>0.052985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ingroup</th>\n",
       "      <td>0.032067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Purity</th>\n",
       "      <td>0.018520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not scored</th>\n",
       "      <td>-0.849965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">SDO-7</th>\n",
       "      <th>trait antiegalitarianism</th>\n",
       "      <td>-0.972336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trait dominance</th>\n",
       "      <td>-0.682363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      avg_score\n",
       "scale sub_scale                                \n",
       "CSES  Importance to Identity           0.003708\n",
       "      Membership self-esteem.          0.011190\n",
       "      Private collective self-esteem   0.016076\n",
       "      Public collective self-esteem    0.014440\n",
       "IPVAS Control                         -0.633278\n",
       "      Threat                          -0.333069\n",
       "      Violence                        -0.999981\n",
       "LWAI  Anticonventionalism              0.000000\n",
       "      Antihierarchical Aggression      0.000000\n",
       "      Top-Down Censorship              0.000000\n",
       "MFQ   Authority                       -0.017626\n",
       "      Fairness                         0.081313\n",
       "      Harm                             0.052985\n",
       "      Ingroup                          0.032067\n",
       "      Purity                           0.018520\n",
       "      not scored                      -0.849965\n",
       "SDO-7 trait antiegalitarianism        -0.972336\n",
       "      trait dominance                 -0.682363"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# answer_choice_list('Strongly disagree, Somewhat disagree, Neither agree nor disagree, Somewhat agree, Strongly agree')\n",
    "\n",
    "scales.groupby(['scale', 'sub_scale']).agg(avg_score = ('model_score', 'mean'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa29a038",
   "metadata": {},
   "source": [
    "## quick steering test\n",
    "\n",
    "### On the instruct model\n",
    "\n",
    "- model steering with a subset of SDO items does affect SDO -- so that's promising! it does immediately pass a really basic/naive test.\n",
    "- AND, it also spills over to substantailly affect IPVAS, suggesting some generalization.\n",
    "\n",
    "\n",
    "### on the non-instruct model\n",
    "\n",
    "- seemingly it is not affected, which is strange. Also scores very strangely in raw tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "57c1c4f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>avg_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scale</th>\n",
       "      <th>sub_scale</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">CSES</th>\n",
       "      <th>Importance to Identity</th>\n",
       "      <td>0.009327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Membership self-esteem.</th>\n",
       "      <td>0.004570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Private collective self-esteem</th>\n",
       "      <td>-0.011315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Public collective self-esteem</th>\n",
       "      <td>0.032840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">IPVAS</th>\n",
       "      <th>Control</th>\n",
       "      <td>-0.508341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Threat</th>\n",
       "      <td>0.052839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Violence</th>\n",
       "      <td>-0.741021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">LWAI</th>\n",
       "      <th>Anticonventionalism</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Antihierarchical Aggression</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Top-Down Censorship</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">MFQ</th>\n",
       "      <th>Authority</th>\n",
       "      <td>-0.002137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fairness</th>\n",
       "      <td>0.001122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Harm</th>\n",
       "      <td>0.001027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ingroup</th>\n",
       "      <td>-0.001178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Purity</th>\n",
       "      <td>-0.001007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not scored</th>\n",
       "      <td>-0.608582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">SDO-7</th>\n",
       "      <th>trait antiegalitarianism</th>\n",
       "      <td>-0.017029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trait dominance</th>\n",
       "      <td>-0.027916</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      avg_score\n",
       "scale sub_scale                                \n",
       "CSES  Importance to Identity           0.009327\n",
       "      Membership self-esteem.          0.004570\n",
       "      Private collective self-esteem  -0.011315\n",
       "      Public collective self-esteem    0.032840\n",
       "IPVAS Control                         -0.508341\n",
       "      Threat                           0.052839\n",
       "      Violence                        -0.741021\n",
       "LWAI  Anticonventionalism              0.000000\n",
       "      Antihierarchical Aggression      0.000000\n",
       "      Top-Down Censorship              0.000000\n",
       "MFQ   Authority                       -0.002137\n",
       "      Fairness                         0.001122\n",
       "      Harm                             0.001027\n",
       "      Ingroup                         -0.001178\n",
       "      Purity                          -0.001007\n",
       "      not scored                      -0.608582\n",
       "SDO-7 trait antiegalitarianism        -0.017029\n",
       "      trait dominance                 -0.027916"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdo = scales.loc[lambda x: ((x.scale == \"SDO-7\") & (x.direction == 'high'))]\n",
    "sdo_zipped = zip(sdo.original_statement.to_list(), sdo.contrastive_statement.to_list())\n",
    "sdo_vec = get_mean_steering_vector(sdo_zipped, wmodel)\n",
    "\n",
    "## Getting model responses\n",
    "\n",
    "resps = []\n",
    "resp_probs = []\n",
    "\n",
    "for idx, row in scales.iterrows():\n",
    "\n",
    "    with Trace(wmodel.get_module(), edit_output = act_add(2*steering_vec)):\n",
    "        choices = re.split(';\\W', row['response'])\n",
    "        choices = [c.strip() for c in choices]\n",
    "        choice_probs = mc_choice_probs(wmodel, format_chat(row['original_statement']), choices) # format_chat for instruct model\n",
    "\n",
    "        resp_probs.append(choice_probs.detach().numpy())\n",
    "        resps.append(choice_score(choice_probs) if row['direction']=='high' else -choice_score(choice_probs))\n",
    "    \n",
    "scales[\"response_probs\"] = resp_probs\n",
    "scales[\"model_score\"] = resps\n",
    "\n",
    "scales.groupby(['scale', 'sub_scale']).agg(avg_score = ('model_score', 'mean'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f26699",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
